[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos de ANOVA e Regressão Linear",
    "section": "",
    "text": "Apresentação\n\n\n\n\n\n\nNota\n\n\n\nAo apresentar data.frames como resultados das análises de dados, utilizaremos o pacote flextable para fins de apresentação gráfica. Caso deseje rodar os códigos no RStudio, remova as funções flextable() e outras funções associadas (ex.: fit_to_width(max_width = 8)).\nOs pacotes utilizados aparecem ao início de cada capítulo."
  },
  {
    "objectID": "qmd/01-anova/01-anova.html#o-modelo-da-anova-e-as-hipóteses-estatísticas",
    "href": "qmd/01-anova/01-anova.html#o-modelo-da-anova-e-as-hipóteses-estatísticas",
    "title": "1  Análise de variância de um fator",
    "section": "1.1 O modelo da ANOVA e as hipóteses estatísticas",
    "text": "1.1 O modelo da ANOVA e as hipóteses estatísticas\nO modelo pode ser representado por:\n\\[Y_{ij} = \\mu + A_i + \\epsilon_{ij}\\]\nonde \\(Y_{ij}\\) é a variável resposta associada à observação \\(i\\) do tratamento \\(j\\), \\(\\mu\\) representa a média geral e \\(A_i\\) o efeito do tratamento \\(i\\). O termo \\(\\epsilon_{ij}\\) é denominado de resíduo (ou erro) associado a cada observação, que assumimos ter distribuição normal com média zero e variância constante.\n\\[\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\n\n\n\n\nHipóteses estatísticas no modelo de ANOVA\n\n\n\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3 =.... = \\mu_k\\) (HIPÓTESE NULA)\n\\(H_a\\): ao menos um par de médias é diferente (HIPÓTESE ALTERNATIVA)\n\n\nA hipótese nula (\\(H_0\\)) define a ausência de diferenças entre as médias populacionais enquanto a hipótese alternativa (\\(H_a\\)) refere-se a qualquer possibilidade diferente de \\(H_0\\). Se temos exatamente dois níveis em \\(X\\), a comparação de médias pode ser feita por meio de um teste \\(t\\). A ANOVA deve ser utilizada quando temos mais de dois níveis em \\(X\\). Neste sentido, o teste \\(t\\) é um caso particular da ANOVA."
  },
  {
    "objectID": "qmd/01-anova/01-anova.html#partição-das-soma-dos-quadrados",
    "href": "qmd/01-anova/01-anova.html#partição-das-soma-dos-quadrados",
    "title": "1  Análise de variância de um fator",
    "section": "1.2 Partição das Soma dos Quadrados",
    "text": "1.2 Partição das Soma dos Quadrados\nAo representarmos a distribuição de uma variável \\(Y\\) contínua em função de uma variável \\(X\\) categórica, geralmente estamos interessados em determinar se os diferentes níveis de \\(X\\) (diferentes grupos) têm médias similares ou se ao menos um dos níveis têm média diferente dos demais. Queremos uma medida que nos permita diferenciar situações como as apresentadas abaixo.\n\n\n\n\n\n\n\n\n\n\n\n\nNa figura \\(A\\) todos os grupos são provenientes da mesma distribuição e têm médias aproximadamente iguais (\\(\\overline{Y}_A \\approx \\overline{Y}_B \\approx \\overline{Y}_C \\approx \\overline{Y}_D\\)). Na figura \\(B\\) o segundo grupo tem média mais elevada que os demais, e na da figura \\(C\\), todas as médias parecem ser diferentes entre si (\\(\\overline{Y}_A \\ne \\overline{Y}_B \\ne \\overline{Y}_C \\ne \\overline{Y}_D\\)).\nPara mensurar o grau de associação entre \\(Y\\) e \\(X\\) e entender como podemos diferenciar as situações acima, vamos introduzir o processo de Partição da Soma dos Quadrados.\nSuponha a situção abaixo:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotações\n\n\n\n\nTemos \\(k = 3\\) grupos (A, B ou C) e para cada grupo \\(n = 5\\) observações. Denotamos por \\(n_{ij}\\) o número de observações dentro de cada grupo, em que \\(i\\) é a i-ésima observação (\\(i = 1\\) a \\(5\\)) do j-ésimo grupo (\\(j = 1\\) a \\(3\\) - grupos A ao C). Neste exemplo, o número de observações em cada grupo é o mesmo (\\(n_1 = n_2 = n_3 = n\\)), de modo que o total de observações é dado por:\n\n\\(N = k \\times n = n_1 + n_2 + n_3 = 15\\)\n\nA média de cada grupo será denotada por \\(\\overline{Y}_j\\), que neste exemplo são: \\(Y_1 = 20.64\\) (grupo A), \\(Y_2 = 28.68\\) (grupo B) e \\(Y_3 = 12.18\\) (grupo C).\nVamos denotar por \\(\\overline{\\overline{Y}}\\) a Grande Média, isto é, a média geral de todas as observações independente do grupo de origem.\n\n\\[\\overline{\\overline{Y}} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}\\frac{Y_{ij}}{N} = \\frac{\\overline{Y_1} + \\overline{Y_2} + \\overline{Y_3}}{3} = 20.5\\]\n\n\nPodemos agora observar estes elementos no gráfico de dispersão.\n\n\n\n\n\n\n\n\n\nEm seguida, precisamos calcular \\(3\\) quantias, a Soma dos Quadrados Totais (\\(SQ_{Total}\\)), a Soma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\) e a Soma dos Quadrados dos Resíduos \\(SQ_{Res}\\).\n\nSoma dos Quadrados Totais \\(SQ_{Total}\\): mede as diferenças entre \\(Y_{ij}\\) e \\(\\overline{\\overline{Y}}\\). Temos nesta expressão o somatório dos desvios ao quadrado de todas as observações com relação à grand, fig.align=‘center’, fig.width=8, fig.height=4e média independente do grupo de origem de cada observação.\n\n\\[SQ_{Total} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2\\]\n\nSoma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\): mede as diferenças entre as médias dos tratamentos \\(\\overline{Y}_j\\) e \\(\\overline{\\overline{Y}}\\), sendo portanto os desvios ao quadrado da média de cada tratamento subtraída da grande média. \\(SQ_{Trat}\\) também é chamada de soma dos quadrados entre grupos ou entre tratamentos\n\n\\[SQ_{Trat} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2\\]\n\nSoma dos Quadrados dos Resíduos \\(SQ_{Res}\\): mede as diferenças entre cada observação \\(Y_{ij}\\) e a média de seu próprio grupo \\(\\overline{Y}_{j}\\). \\(SQ_{Res}\\) também é chamada de soma dos quadrados dentro dos grupos ou dentro dos tratamentos\n\n\\[SQ_{Res} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(Y_{ij} - \\overline{Y}_{j})^2\\]\n\n\n\n\n\n\nA característica aditiva das somas dos quadrados\n\n\n\nA partição da soma dos quadrados consiste em decompor a variação total do experimento em uma parcela atribuída à variação entre tratamentos e outra parcela da variação dentro dos tratamentos. Isto é possível pois as somas dos quadrados definidas acima podem ser expressas de forma aditiva como:\n\\[SQ_{Total} = SQ_{Trat} + SQ_{Res}\\]\nDeste modo, é possível demostrar que:\n\\(\\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(Y_{j} - \\overline{\\overline{Y}})^2 + \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{Y}_{j})^2\\)"
  },
  {
    "objectID": "qmd/01-anova/01-anova.html#medindo-a-associação-entre-y-e-x",
    "href": "qmd/01-anova/01-anova.html#medindo-a-associação-entre-y-e-x",
    "title": "1  Análise de variância de um fator",
    "section": "1.3 Medindo a associação entre \\(Y\\) e \\(X\\)",
    "text": "1.3 Medindo a associação entre \\(Y\\) e \\(X\\)\nA característica aditiva das somas dos quadrados pode ser utilizada para mensurar o grau de dependência de \\(Y_{ij}\\) com respeito aos diferentes tratamentos. Compare as duas figuras abaixo:\n\n\n\n\n\n\n\n\n\n\n\n\nA soma dos quadrados dentro dos grupos é a mesma nas duas figuras (\\(SQ_{Res} = 362.6\\)). No entanto, na figura da esquerda, em que as médias dos tratamentos são similares (e consequentemente próximas à grande média), a soma dos quadrados entre os tratamentos é muito menor (\\(SQ_{Trat}^{esquerda} = 15.8\\)) que na figura da direita, em que as médias dos tratamentos estão distantes entre si (\\(SQ_{Trat}^{direita} = 680.8\\)). É desta forma que a partição das somas dos quadrados nos permite diferenciar situações em que: i - a média dos grupos depende dos níveis do tratamento (figura da direita); de situações em que ii - a média não depende dos níveis do tratamento (figura da esquerda)."
  },
  {
    "objectID": "qmd/01-anova/01-anova.html#quadrados-médios-e-graus-de-liberdade",
    "href": "qmd/01-anova/01-anova.html#quadrados-médios-e-graus-de-liberdade",
    "title": "1  Análise de variância de um fator",
    "section": "1.4 Quadrados médios e graus de liberdade",
    "text": "1.4 Quadrados médios e graus de liberdade\nPara que os somatórios dos quadrados expressem uma medida de variação é necessário corriglos em função dos graus de liberdade (\\(gl\\)), obtendo assim Quadrados médios dados abaixo:\n\nQuadrado Médio Total (\\(QM_{Total}\\))\n\n\\[QM_{Total} = \\frac{SQ_{Total}}{gl_{Total}}\\]\nem que \\(gl_{Total} = N - 1\\)\n\nQuadrado Médio dos Tratamentos (\\(QM_{Trat}\\))\n\n\\[QM_{Trat} = \\frac{SQ_{Trat}}{gl_{Trat}}\\]\nem que \\(gl_{Trat} = k - 1\\)\n\nQuadrado Médio dos Resíduos (\\(QM_{Res}\\))\n\n\\[QM_{Res} = \\frac{SQ_{Res}}{gl_{Res}}\\]\nem que \\(gl_{Res} = N-k\\)\nAssim como a soma dos quadrados, os graus de liberdade também têm característica aditiva.\n\\[gl_{Total} = gl_{Trat} + gl_{Res} = (k - 1) + (N - K) = N - 1\\]\nOs quadrados médios que são estimativas de variâncias. Compare por exemplo a expressão do \\(QM_{Total}\\) com a fórmula da variância amostral (\\(s^2\\))) e verá que excetuando mudanças de notação, as expressões são essencialmente as mesmas."
  },
  {
    "objectID": "qmd/01-anova/01-anova.html#estatística-f-e-teste-de-hipóteses",
    "href": "qmd/01-anova/01-anova.html#estatística-f-e-teste-de-hipóteses",
    "title": "1  Análise de variância de um fator",
    "section": "1.5 Estatística \\(F\\) e teste de hipóteses",
    "text": "1.5 Estatística \\(F\\) e teste de hipóteses\nUma vez que os quadrados médios são estimativas de variância, uma estatística de teste apropriada é:\n\\[F_{calculado} = \\frac{QM_{Trat}}{QM_{Res}}\\]\nA estatística \\(F\\) (ou razão-\\(F\\)) está associada à distribuição de probabilidades \\(F\\) e nos permite comparar a variância associada ao tratamento com a variância associada aos resíduos. Em mãos do valor de \\(F_{calculado}\\), o teste de hipóteses é possível após a definição do nível de significância \\(\\alpha\\).\n\n1.5.1 Nível de significância\nAssim como discutimos nos testes \\(Z\\) e \\(t\\), o valor de \\(\\alpha\\) estabelece um limite de aceitação para \\(H_0\\), isto é, um limite a partir do qual a estatística do teste se torna tão extrema que nos leva a assumir que \\(H_0\\) é improvável, devendo portanto ser rejeitada em favor de \\(H_a\\). Este passo é possível pois o valor de \\(F_{calculado}\\) pode ser associado à distribuição \\(F\\) de probabilidades, o que nos permite calcular a probabilidade:\n\\[P(F_{calculado}) \\le \\alpha\\]\nPara facilitar a notação denominaremos \\(P(F_{calculado})\\) simplesmente de valor de \\(p\\) expresso em vermelho na figura abaixo:\n\n\n\n\n\n\nTomada de decisão na ANOVA\n\n\n\nSe \\(p > \\alpha\\) –> ACEITAMOS \\(H_0\\)\nSe \\(p \\le \\alpha\\) –> REJEITAMOS \\(H_0\\) (e assumimos \\(H_a\\) como verdadeira)\n\n\n\n\n\n\n\n\n\n\n\nTradicionalmente utiliza-se \\(\\alpha = 0.05\\). Neste caso, \\(H_0\\) seria rejeitada somente de \\(p \\le 0.05\\). Algumas área da medicina por eoutro lado, são tradicionais por utilizar valores de \\(\\alpha = 0.01\\), o que torna o experimento menos sujeito ao erro do tipo I. Portanto, outros valores de \\(\\alpha\\) diferentes de \\(0.05\\) podem ser escolhidos. O fundamental é que esta decisão, isto é, sobre o nível de significância \\(\\alpha\\) a ser adotado, seja feita previamente à obtenção dos dados."
  },
  {
    "objectID": "qmd/01-anova/01-anova.html#um-exemplo-de-anova-os-níveis-de-metais-pesados-afetam-a-diversidade-de-espécies",
    "href": "qmd/01-anova/01-anova.html#um-exemplo-de-anova-os-níveis-de-metais-pesados-afetam-a-diversidade-de-espécies",
    "title": "1  Análise de variância de um fator",
    "section": "1.6 Um exemplo de ANOVA: os níveis de metais pesados afetam a diversidade de espécies?",
    "text": "1.6 Um exemplo de ANOVA: os níveis de metais pesados afetam a diversidade de espécies?\nA base de dados medley.csv (disponível também em Chapter 10 - Single factor classification (ANOVA)) nos permitirá testar a hipótese de que a presença de metais pesados afeta a diversidade de espécies de diatomácias em riachos (Medley and Clements (1998); Queen, Quinn, and Keough (2002); Logan (2011)).\n\n\n\n\n\nCode\nmedley = read_csv(\"medley.csv\") %>% \n  mutate(STREAM = factor(STREAM),\n         ZINC = factor(ZINC, ordered = TRUE,\n                       levels = c(\"BACK\", \"LOW\", \n                                  \"MED\", \"HIGH\")))\nmedley %>% flextable()\n\n\n\n\n\nSTREAMZINCDIVERSITYEagleBACK2.27EagleHIGH1.25EagleHIGH1.15EagleMED1.62BlueBACK1.70BlueHIGH0.63BlueBACK2.05BlueBACK1.98BlueHIGH1.04BlueMED2.19BlueMED2.10SnakeBACK2.20SnakeMED2.06SnakeHIGH1.90SnakeHIGH1.88SnakeHIGH0.85ArkanLOW1.40ArkanLOW2.18ArkanLOW1.83ArkanLOW1.88ArkanMED2.02ArkanMED1.94ArkanLOW2.10ChalkLOW2.38ChalkHIGH1.43ChalkHIGH1.37ChalkMED1.75ChalkLOW2.83SplatBACK1.53SplatBACK0.76SplatMED0.80SplatLOW1.66SplatMED0.98SplatBACK1.89\n\n\nA coluna STREAM é uma variável categórica contendo o nome dos \\(6\\) riachos amostrados (Arkan, Blue, Chalk, Eagle, Snake, Splat). A coluna ZINC é uma variável categórica ordinal com \\(4\\) níveis de concentração de zinco na água (BACK < LOW < MED < HIGH). O primeiro nível (BACK) é o nível de referência (BACKGROUND). Finalmente, a coluna DIVERSITY é uma variável contínua que contém a diversidade de diatomácieas (medida pelo índice de diversidade de Shannon medida de cada uma das 34 amostras.\nVamos nos concentrar nas variáveis DIVERSITY e ZINC. DIVERSITY será a variável resposta. Em delineamento experimental, dizemos que ZINC é um tratamento, isto é, uma condição experimental sob a qual nossa variável dependente \\(Y\\) foi mensurada.\nPara verificarmos a distribuição de diversidade para cada concentração de zinco vamos fazer um boxplot da variável DIVERSITY em função de ZINC.\n\n\nCode\nggplot(medley) +\n  aes(x = ZINC, y = DIVERSITY) +\n  geom_boxplot(coef = 3) +\n  theme_classic(base_size = 15)\n\n\n\n\n\nVemos que a concentração alta aparenta ter menor diversidade que as demais concentralções. A ANOVA nos permitirá testar esta suposição.\n\n\n\n\n\n\nHipópteses estatísticas\n\n\n\n\\(H_0: \\mu_{BACK} = \\mu_{LOW} = \\mu_{MED} = \\mu_{HIGH}\\)\n\\(H_a\\): ao menos um \\(\\mu\\) é diferente\n\\(\\alpha = 0.05\\)\n\n\n\n1.6.1 Calculando a ANOVA\n\n\n\ni. Somatórios dos quadrados\n\\(SQ_{Trat} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2 = 2.5666124\\)\n\\(SQ_{Res} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(Y_{ij} - \\overline{Y}_{j})^2 = 6.5164111\\)\nii. Graus de liberdade\n\\(gl_{Trat} = k - 1 = 3\\)\n\\(gl_{Res} = N-k = 30\\)\niii. Quadrados médios\n\\(QM_{Trat} = \\frac{SQ_{Trat}}{gl_{Trat}} = 0.8555375\\)\n\\(QM_{Res} = \\frac{SQ_{Res}}{gl_{Res}} = 0.2172137\\)\niv. Estatística \\(F\\)\n\\(F_{calculado} = \\frac{QM_{Trat}}{QM_{Res}} = 3.939\\)\n\n\n\n\n\n\nTabela da ANOVA**\n\n\n\nAs quantias acima são tradicionalmente expressas em uma Tabela de ANOVA.\n\n\nCode\nanova_ex = anova(aov(DIVERSITY ~ ZINC, data = medley))\n\n\n\n\n\n\nTabela 1.1:  Tabela da ANOVA para a base de dados medley. DfSum SqMean SqF valuePr(>F)32.5666120.85553753.938690.01755956306.5164110.2172137\n\n\n\nem que:\nDf: graus de liberdade\nSum Sq: soma dos quadrados\nMean Sq: quadrados médios\nF value: valor de \\(F_{calculado}\\)\nPr(>F): valor de p\nA primeira linha refere-se aos valores associados aos tratamentos e a segunda linha aos resíduos. Note que o cômputo de \\(SQ_{Total}\\), \\(gl_{Total}\\) e \\(QM_{Total}\\) não é realmente necessário.\n\n\nO valor de \\(p = 3.9386901\\) mostrado na Tabela 1.1 acima é refere-se à área na distribuição \\(F\\) que fica acima de \\(F_{calculado}\\). Poderíamos tentar representar este valor visualmente na distribuição \\(F\\), mas ele é tão pequeno, que a área em vermelho sequer aparece na figura.\n\n\n\n\n\n\n\n\n\nComo conclusão temos que \\(p \\le \\alpha\\) nos leva a REJEITAR \\(H_0\\), pois \\(F_{calculado}\\) é muito extremo para ser resultante da hipótese nula. Neste caso, assumimos que a \\(H_a\\) é mais condizente com a estrutura dos dados, de modo que os tratamentos devem ser provenientes de populações estatísticas com diferentes médias \\(\\mu\\)."
  },
  {
    "objectID": "qmd/01-anova/01-anova.html#testes-a-posteriori-de-comparação-de-médias",
    "href": "qmd/01-anova/01-anova.html#testes-a-posteriori-de-comparação-de-médias",
    "title": "1  Análise de variância de um fator",
    "section": "1.7 Testes a posteriori de comparação de médias",
    "text": "1.7 Testes a posteriori de comparação de médias\nTendo rejeitado \\(H_0\\) concluímos que ao menos 1 par médias é diferente entre si, saber qual(is). Isto nos leva a buscar por um teste que permita fazer comparações par-a-par. Os testes a posteriori são uma alternativa.\nEntre os diferentes testes a posteriori na literatura discutiremos o teste de Tukey, em que o objetivo é estabelecer uma Diferença Honesta Significativa (DHS) entre um dado par de médias. Esta diferença pode ser calculada por:\n\\[DHS_{12} = q\\sqrt{\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)QM_{Res}}\\]\nonde:\n\\(q\\): é um valor retirado de uma tabela estatística da distribuição de amplitude normalizada (studentized range q table). Para um dado \\(\\alpha\\), o valor desejado de \\(q\\) é encontrado cruzando a linha contento o número \\(k\\) de tratamentos do experimento com a linha contendo os graus de liberdade do resíduo (\\(gl_{Res}\\)). Veja um exemplo desta tabela no link: Studentized Range q Table;\n\\(QM_{Res}\\): é quadrado médio do resíduo obtido na ANOVA, e;\n\\(n_1\\), \\(n_2\\): os tamanhos amostrais de cada grupo envolvido na comparação.\nPara um dado nível de significância \\(\\alpha\\), a \\(DHS\\) irá depender basicamente da variação residual do modelo de ANOVA e dos tamanhos amostrais de cada grupo. Em um experimento balanceado, isto é, onde \\(n_1 = n_2 = \\cdots = n_k = n\\), a diferença mínima para que um par de médias seja cosiderado diferente é sempre a mesma.\nEm nosso exemplo, a \\(DHS\\) com \\(\\alpha = 0.05\\) irá depender se:\n\nestamos comparando pares em que os dois grupos têm \\(n = 8\\) (BACK e LOW):\n\n\n\n\n\\(DHS = 3.845\\sqrt{\\left(\\frac{1}{8} + \\frac{1}{8}\\right)0.2172137} = 0.896\\)\n\nestamos comparando pares em que os dois grupos têm \\(n = 9\\) (MED e HIGH):\n\n\n\n\n\\(DHS = 3.845\\sqrt{\\left(\\frac{1}{9} + \\frac{1}{9}\\right)0.2172137} = 0.845\\)\n\nestamos comparando pares em que um grupo tem \\(n = 8\\) (BACK e LOW) e o outro \\(n = 9\\) (MED e HIGH):\n\n\n\n\n\\(DHS = 3.845\\sqrt{\\left(\\frac{1}{8} + \\frac{1}{9}\\right)0.2172137} = 0.871\\)\nDe todo modo, note que as diferenças ficam entre 0.845 e 0.896.\nEm nosso exemplo, as médias dos grupos foram:\n\n\nCode\nDm = medley %>% \n  group_by(ZINC) %>% \n  summarize(Medias = mean(DIVERSITY))\n\n\n\n\\(\\overline{Y}_{BACK} = 1.7975\\)\n\\(\\overline{Y}_{LOW} = 2.0325\\)\n\\(\\overline{Y}_{MED} = 1.7177778\\)\n\\(\\overline{Y}_{HIGH} = 1.2777778\\)\n\nE as diferenças (\\(\\overline{Y}_{maior} - \\overline{Y}_{menor}\\)) entre elas:\n\n\n\nZINCBACKLOWMEDHIGHBACK0.00000000LOW0.235000000.0000000MED0.079722220.31472220.00HIGH0.519722220.75472220.440"
  },
  {
    "objectID": "qmd/01-anova/01-anova.html#ajustando-a-anova-no-r",
    "href": "qmd/01-anova/01-anova.html#ajustando-a-anova-no-r",
    "title": "1  Análise de variância de um fator",
    "section": "1.8 Ajustando a ANOVA no R",
    "text": "1.8 Ajustando a ANOVA no R\nConsidere que a tabela em nosso exemplo está no objeto Tab. A ANOVA no R é feita com o comando aov.\n\najuste = aov(DIVERSITY ~ ZINC, data = medley)\najuste\n\nCall:\n   aov(formula = DIVERSITY ~ ZINC, data = medley)\n\nTerms:\n                    ZINC Residuals\nSum of Squares  2.566612  6.516411\nDeg. of Freedom        3        30\n\nResidual standard error: 0.4660619\nEstimated effects may be unbalanced\n\n\n\nA notação Y ~ X será muito utilizada nesta seção sobre modelos lineares e lê-se como \\(Y\\) é função de \\(X\\).\n\nO comando acima fez os cálculos da ANOVA, isto é, computou as somas dos quadrados, os graus de liberdade, os quadrados médios, o \\(F_{calculado}\\) e o valor de \\(p\\). Para visualizarmos a tabela da ANOVA fazemos:\n\nanova(ajuste)\n\nAnalysis of Variance Table\n\nResponse: DIVERSITY\n          Df Sum Sq Mean Sq F value  Pr(>F)  \nZINC       3 2.5666 0.85554  3.9387 0.01756 *\nResiduals 30 6.5164 0.21721                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote que os resultados coincidem com o que apresentamos anteriormente. Como o valor de \\(p\\) foi menor que \\(\\alpha = 0.05\\), concluimos que a ANOVA foi significativa, isto é, indicou que ao menos um par de médias difere ente si. Podemos fazer o teste a posteriori de Tukey com o comando:\n\nalfa = 0.05\nTukeyHSD(ajuste, conf.level = 1-alfa)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = DIVERSITY ~ ZINC, data = medley)\n\n$ZINC\n                 diff        lwr         upr     p adj\nLOW-BACK   0.23500000 -0.3986367  0.86863665 0.7457444\nMED-BACK  -0.07972222 -0.6955064  0.53606192 0.9847376\nHIGH-BACK -0.51972222 -1.1355064  0.09606192 0.1218677\nMED-LOW   -0.31472222 -0.9305064  0.30106192 0.5153456\nHIGH-LOW  -0.75472222 -1.3705064 -0.13893808 0.0116543\nHIGH-MED  -0.44000000 -1.0373984  0.15739837 0.2095597\n\n\nO resultado apresenta todas as comparações possíveis entre os grupos, mostrando as diferenças de médias, seus intervalos de confiança a \\(95\\%\\) e os valores de \\(p\\), indicando quais destas diferenças são significativas, isto é, \\(p \\le \\alpha\\). Estes resultados nos permitem concluir novamente que somente o par HIGH-LOW difere entre si, pois p adj < 0.05.\nUm gráfico facilita a visualização das comparações, sobretudo em situações com muitos pares de médias envolvidos:\n\nplot(TukeyHSD(ajuste))\n\n\n\n\nNeste gráfico, as comparações em que o intervalo de confiança não inclui o zero, são consideradas significativas. Novamente, vemos que somente o grupo C-B tem médias estatisticamente diferentes."
  },
  {
    "objectID": "qmd/01-anova/01-anova.html#pressupostos-da-anova",
    "href": "qmd/01-anova/01-anova.html#pressupostos-da-anova",
    "title": "1  Análise de variância de um fator",
    "section": "1.9 Pressupostos da ANOVA",
    "text": "1.9 Pressupostos da ANOVA\nOs pressupostos da ANOVA são:\n\nOs observação são independentes e;\nA variância dos resíduos é homogênea e;\nOs resíduos têm distribuição normal com média \\(0\\) e variância \\(\\sigma^2\\).\n\nVamos inicialmente testar o pressuposto de homogeneidade de variâncias com um teste \\(F\\).\n\nv = tapply(medley$DIVERSITY, medley$ZINC, var)\nvar_max = max(v)\nvar_min = min(v)\n\n\nmedley %>% group_by(ZINC) %>% \n  summarise(Var = var(DIVERSITY))\n\nNote que a maior variância é \\(0.2530194\\) e a menor \\(0.1822194\\).\nO teste \\(F\\) consiste em dividir a maior variância pela menor:\n\nvar.test(medley$DIVERSITY[medley$ZINC == \"MED\"], medley$DIVERSITY[medley$ZINC == \"HIGH\"])\n\n\n    F test to compare two variances\n\ndata:  medley$DIVERSITY[medley$ZINC == \"MED\"] and medley$DIVERSITY[medley$ZINC == \"HIGH\"]\nF = 1.3885, num df = 8, denom df = 8, p-value = 0.6534\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3132103 6.1557698\nsample estimates:\nratio of variances \n          1.388543 \n\n\nA maior variância foi 1.39 vezes maior que a menor variância e o test F sugere que esta diferença é não-significativa a \\(5\\%\\) (\\(p < 0.05\\)). Isto indica que as variâncias são homogêneas.\nA verificação visual de que as variâncias são homogêneas pode também ser inspecionada pelo gráfico de resíduos:\n\nplot(rstudent(ajuste) ~ fitted(ajuste), pch = 16)\nabline(h = 0, col = 2)\n\n\n\n\nEm seguida avaliamos o histograma dos resíduos e aplicamos um teste de normalidade (ex. teste de Shapiro-Wilk) para verificar se o pressuposto de normalidade pode ser aceito.\n\nhist(rstudent(ajuste), breaks = 10)\n\n\n\nshapiro.test(rstudent(ajuste))\n\n\n    Shapiro-Wilk normality test\n\ndata:  rstudent(ajuste)\nW = 0.96696, p-value = 0.3828\n\n\nNeste caso, o valor de \\(p > 0.05\\) indica não haver desvio da normalidade.\n\n\n\n\nLogan, Murray. 2011. Biostatistical Design and Analysis Using r: A Practical Guide. John Wiley & Sons.\n\n\nMedley, C. N., and W. H. Clements. 1998. “Responses of Diatom Communities to Heavy Metals in Streams: The Influence of Longitudinal Variation.” Ecological Applications 9: 631–44.\n\n\nQueen, Jerry P, Gerry P Quinn, and Michael J Keough. 2002. Experimental Design and Data Analysis for Biologists. Cambridge university press."
  },
  {
    "objectID": "qmd/02-regressao/01-regressao_simples.html#modelo-geral-de-regressão",
    "href": "qmd/02-regressao/01-regressao_simples.html#modelo-geral-de-regressão",
    "title": "2  Regressão Linear Simples",
    "section": "2.1 Modelo geral de regressão",
    "text": "2.1 Modelo geral de regressão\nA estrutura de um modelo de regressão é dada por:\n\\[Y_i = f(X_i, \\beta) + \\epsilon_i\\]\nonde \\(f(X_i, \\beta)\\) representa a parte determinística e \\(\\epsilon\\) a parte estocástica. O sulfixo i nos diz que esta expressão é dada para cada par de observação \\((Y,X)\\).\n\n2.1.1 Porção determinística\nA porção determinística é um modelo matemático que descreve a relação funcional entre \\(X\\) e \\(Y\\). Os parâmetros \\(\\beta\\)’s determinam a intensidade do efeito de \\(X\\) sobre \\(Y\\). Na regressão linear simples temos somente uma variável \\(X\\), e a relação funcional é dada pela equação da reta. No modelo de regressão linear múltipla existe mais de uma variável \\(X\\). Finalmente, nos modelos de regressão não-lineares a relação funcional pode ser representada por outros modelos matemáticos (ex. função potência \\(Y = \\beta_0X^{\\beta_1}\\)).\nNa regressão linear simples, o parâmetro \\(\\beta_1\\) é geralmente o de maior interesse. Este parâmetro nos dirá se a relação será crescente (\\(\\beta_1 > 0\\)), decrescente (\\(\\beta_1 < 0\\)) ou nula (\\(\\beta_1 = 0\\)). \\(\\beta_0\\) é o \\(\\textbf{intercepto}\\) e expressa o ponto em \\(Y\\) em que a reta cruza o eixo das ordenadas.\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 Porção estocástica\nA porção estocástica, é representada pelo resíduo ou erro. A cada observação \\(Y_i\\) está associado um valor de resíduo correspondente (\\(\\epsilon_i\\)), dado pela distância vertical entre \\(Y_i\\) e o valor predito \\(\\hat{Y_i}\\) sobre a reta de regressão.\n\n\n\n\n\n\n\n\n\nNo modelo de regressão linear que veremos aqui, os resídos são uma variável aleatória prevenientes de uma distribuição normal de probabilidades com média \\(\\mu = 0\\) e variância \\(\\sigma^2\\) constante ao longo da reta de regressão, \\(N(0, \\sigma^2)\\).\n\n\n\nFigura 2.1: Resíduo normalmente distribuído ao longo da reta de regressão."
  },
  {
    "objectID": "qmd/02-regressao/01-regressao_simples.html#ajuste-dos-dados-ao-modelo-de-regressão",
    "href": "qmd/02-regressao/01-regressao_simples.html#ajuste-dos-dados-ao-modelo-de-regressão",
    "title": "2  Regressão Linear Simples",
    "section": "2.2 Ajuste dos dados ao modelo de regressão",
    "text": "2.2 Ajuste dos dados ao modelo de regressão\nO ajuste de dados observados a um modelo de regressão requer a obtenção de estimativas para \\(\\beta_0\\), \\(\\beta_1\\) e \\(\\sigma^2\\), denotadas respectivamente por \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}^2\\). Note que o símbolo \\(\\hat{}\\) significa que estamos falando de estimativas obtidas a partir de dados amostrais e não dos parâmetros populacionais.\nAo obter estas estimativas, podemos encontrar valores ajustados de \\(Y\\) para um dados valor de \\(X\\). Os valores ajustados de \\(Y\\) são denotados por \\(\\hat{Y}\\).\n\\[\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}X_i\\]\n\n2.2.1 Método dos mínimos quadrados\nO Método dos Mínimos Quadrados (\\(MMQ\\)) é uma das formas disponíveis para calcularmos \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}^2\\). O \\(MMQ\\) envolve encontrar a combinação de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\) que minimiza a Soma dos Quadrados dos Resíduos (\\(SQ_{Resíduo}\\)), ou seja, que minimizam a quantia:\n\\[SQ_{Resíduo} = \\sum{(Y_i-\\hat{Y_ i})^2} = \\sum{(Y_i-(\\hat{\\beta_0} + \\hat{\\beta_1}X_i))^2}\\]\n\n\n\n\n\n\n\n\n\nNas figuras acima, a linha da esquerda (\\(SQ_{Resíduo} = \\sum{\\epsilon_i^2} = 37\\)) está claramente melhor ajustada à nuvem de pontos, o que se expressa em um menor somatório dos quadrados dos resíduos (\\(SQ_{Resíduo} = \\sum{\\epsilon_i^2} = 37\\)) quando comparado com o ajuste da figura à direita (\\(SQ_{Resíduo} = \\sum{\\epsilon_i^2} = 145\\)).\n\n\n2.2.2 Variâncias, covariâncias e coeficientes da regressão\nPara estimarmos os coeficientes da regressão \\(\\beta_0\\) e \\(\\beta_1\\) devemos retomar o conceito de variância amostral e introduzir o conceito de covariância amostral.\nA variância amostral de \\(Y\\) por exemplo, pode ser obtida subtraindo cada observação em \\(Y\\) de sua média (\\(\\overline{Y}\\)) e elevando esta subtração ao quadrado \\((Y_i - \\overline{Y})^2\\). Ao somar para todos os valores de \\(Y_i\\) teremos o somatório dos quadrados de \\(Y\\) (\\(SQ_Y\\)).\n\\[SQ_Y = \\sum_{i-1}^{n} (Y_i - \\overline{Y})^2 = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (Y_i - \\overline{Y})\\]\nDividindo \\(SQ_Y\\) por \\(n-1\\) teremos a variância amostral de \\(Y\\) (\\(s^2_Y\\)).\n\\[s^2_Y = \\frac{\\sum_{i-1}^{n} (Y_i - \\overline{Y})^2}{n-1}\\]\nAdotando o mesmo procedimento para \\(X\\), podemos calcular o somatório dos quadrados de \\(X\\) (\\(SQ_X\\)).\n\\[SQ_X = \\sum_{i-1}^{n} (X_i - \\overline{X})^2 = \\sum_{i-1}^{n}(X_i - \\overline{X}) (X_i - \\overline{X})\\]\ne a variância amostral de \\(X\\) (\\(s^2_X\\)).\n\\[s^2_X = \\frac{\\sum_{i-1}^{n} (X_i - \\overline{X})^2}{n-1}\\]\nCombinando as duas ideias, teremos o produto cruzado de \\(Y\\) e \\(X\\) (\\(SQ_{YX}\\))\n\\[SQ_{YX} = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})\\]\ne a covariância amostral entre \\(Y\\) e \\(X\\) (\\(s_{YX}\\)).\n\\[s_{YX} = \\frac{\\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})}{n-1}\\]\nO estimador \\(\\hat{\\beta_1}\\) nada mais é que a covariância entre \\(Y\\) e \\(X\\) padronizada pela variância de \\(X\\).\n\\[\\hat{\\beta_1} = \\frac{s_{YX}}{s^2_X} = \\frac{\\frac{SQ_{XY}}{n-1}}{\\frac{SQ_X}{n-1}} = \\frac{SQ_{XY}}{SQ_X} = \\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{\\sum{(X_i - \\overline{X})^2}}\\]\n\\[\\hat{\\beta_1} = \\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{\\sum{(X_i - \\overline{X})^2}}\\]\nApós encontrar \\(\\hat{\\beta_1}\\), podemos calcular \\(\\hat{\\beta_0}\\) sabendo que a melhor reta de regressão passará necessariamente pelo ponto médio de \\(X\\) e de \\(Y\\). Deste modo temos:\n\\[\\hat{\\beta_0} = \\overline{Y} - \\hat{\\beta_1}\\overline{X}\\]\nCalculados \\(\\hat{\\beta_1}\\) e \\(\\hat{\\beta_0}\\), podemos encontrar os valores ajustados de \\(Y\\) para cada valor de \\(X\\) que serão utilizados para construir a reta de regressão. \\(\\hat{Y_i}\\) será dado por:\n\\[\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}X_i\\]\nPor fim, a variância residual \\(s^2\\) é dada por:\n\\[s^2 = QM_{Resíduo} = \\frac{SQ_{Resíduo}}{n-2} = \\frac{\\sum{(Y_i-\\hat{Y_ i})^2}}{n-2}\\]\n\n\n2.2.3 Exemplo de ajuste ao modelo de regressão\n\nrk = read_csv(\"datasets/RIKZ.csv\")\nrks = rk %>% \n  .[seq(3,43,by = 5),]\n\nConsidere a tabela abaixo com os dados de riqueza da macro-fauna praial (número de espécies) e de um índice de exposição às ondas (NAP). Os dados foram obtidos em 2002 na costa da Holanda em nove praias (Zuur et al. 2009). Valores negativos de NAP se referem a locais mais expostos e valores positivos a locais menos expostos à ação das ondas.\n\nrks %>% \n  select(Richness, NAP) %>% \n  flextable()\n\n\nRichnessNAP13-1.33680.6354-0.20130.46060.72912.22211.3757-1.0053-0.002\n\n\nO gráfico de dispersão sugere uma relação negativa e possivelmente linear, em que a riqueza de espécies diminui com o aumento no grau de exposição. Vamos ajustar um modelo de regressão a estes pontos calculando \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}^2\\).\n\n\n\n\n\n\n\n\n\nOs passos intermediários envolvem o cálculo do somatórios dos quadrados de X:\n\\[SQ_X = \\sum{(X_i - \\overline{X})^2}\\]\nde Y:\n\\[SQ_Y = \\sum{(Y_i - \\overline{Y})^2}\\]\ne do somatório dos produtos cruzados de X e Y:\n\\[SQ_{XY} = \\sum{(X_i - \\overline{X}) (Y_i - \\overline{Y})}\\]\nEstes passos são descritos na tabela a seguir.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRichness\nNAP\n\\((X_i - \\overline{X})\\)\n\\((Y_i - \\overline{Y})\\)\n\\((X_i - \\overline{X})^2\\)\n\\((Y_i - \\overline{Y})^2\\)\n\\((X_i - \\overline{X})(Y_i - \\overline{Y})\\)\n\n\n\n\n13\n-1.34\n-1.66\n7.89\n2.74\n62.23\n-13.06\n\n\n8\n0.64\n0.32\n2.89\n0.10\n8.35\n0.91\n\n\n4\n-0.20\n-0.52\n-1.11\n0.27\n1.23\n0.58\n\n\n3\n0.46\n0.14\n-2.11\n0.02\n4.46\n-0.30\n\n\n6\n0.73\n0.41\n0.89\n0.17\n0.79\n0.36\n\n\n1\n2.22\n1.90\n-4.11\n3.62\n16.90\n-7.82\n\n\n1\n1.38\n1.06\n-4.11\n1.11\n16.90\n-4.34\n\n\n7\n-1.00\n-1.32\n1.89\n1.75\n3.57\n-2.50\n\n\n3\n0.00\n-0.32\n-2.11\n0.10\n4.46\n0.68\n\n\n\n\n\nApós os cálculos, os valores estimados são:\n\\[\\hat{\\beta_1} = \\frac{\\sum{(X_i - \\overline{X})(Y_i - \\overline{Y})}}{\\sum{(X_i - \\overline{X})^2}} = \\frac{-25.49}{9.88} = -2.58\\]\n\\[\\hat{\\beta_0} = \\overline{Y} - \\hat{\\beta_1}\\overline{X} = 5.11 -2.58 \\times 0.32 = 5.94\\]\n\\[\\hat{\\sigma}^2 = QM_{Resíduo} = \\frac{SQ_{Resíduo}}{n-2} = \\frac{\\sum{(Y_i-\\hat{Y_ i})^2}}{n-2} = \\frac{53.11}{7} = 7.59\\]\nDe modo que a melhor reta de regressão é dada por:\n\\[Richness = 5.94 -2.58 \\times NAP\\]"
  },
  {
    "objectID": "qmd/02-regressao/01-regressao_simples.html#testes-de-hipóteses-na-regressão-linear-simples",
    "href": "qmd/02-regressao/01-regressao_simples.html#testes-de-hipóteses-na-regressão-linear-simples",
    "title": "2  Regressão Linear Simples",
    "section": "2.3 Testes de hipóteses na regressão linear simples",
    "text": "2.3 Testes de hipóteses na regressão linear simples\nAté o momento, apresentamos uma discussão sobre o método para calcular os estimadores \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}\\). Entretanto, como nossas observações provêm de amostras, estas estimativas estão sujeitas à variação inerente às observações de que dispomos e certamente não serão iguais ao valor da população estatística. Devemos portanto, entender quais evidências estes estimadores nos fornecem para a existência de um efeito de \\(X\\) sobre \\(Y\\), ou seja, para rejeitarmos a hipótese nula em favor de \\(H_A\\).\n\n2.3.1 Teste sobre \\(\\beta_1\\)\nNa regressão linear simples, o efeito de \\(X\\) sobre \\(Y\\) depende do valor de \\(\\beta_1\\)\n\\(Y_i = \\beta + \\beta_1X_i + \\epsilon_i\\)\nA não existência de um efeito implica em \\(\\beta_1 = 0\\) e consequentemente:\n\\(Y_i = \\beta_0 + 0 \\times X_i + \\epsilon_i\\) \\(\\rightarrow\\) \\(Y = \\beta_0 + \\epsilon_i\\)\nPortanto, as hipóteses nula e alternativa seriam:\n\\(H_0: \\beta_1 = 0\\)\n\\(H_A: \\beta_1 \\ne 0\\)\nSegundo \\(H_0\\), a inclinação da reta \\(populacional\\) não é diferente de zero e o valor estimado \\(\\hat{\\beta_1}\\) ocorreu puramente ao acaso, como efeito da variação amostral. Para testar esta hipótese, utilizamos a distribuição de t de modo que:\n\\[t = \\frac{\\hat{\\beta_1} - \\beta_1}{s_{\\hat{\\beta_1}}}\\]\nComo segundo \\(H_0\\), \\(\\beta_1 = 0\\) a expressão fica:\n\\[t = \\frac{\\hat{\\beta_1} - 0}{s_{\\hat{\\beta_1}}} = \\frac{\\hat{\\beta_1}}{s_{\\hat{\\beta_1}}}\\]\n\\(s_{\\hat{\\beta_1}}\\) é o erro padrão de \\(\\beta_1\\) calculado por:\n\\[s_{\\hat{\\beta_1}} = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum{(X_i-\\overline{X})^2}}}\\]\nNo exemplo sobre a fauna praial estamos interessados em testar a hipótese de que a riqueza de espécies esteja associada ao grau de exposição às ondas. Em regressão linear, esta hipótese pode ser expressa por:\n\n\n\n\\[t = \\frac{\\hat{\\beta_1}}{s_{\\hat{\\beta_1}}} = \\frac{-2.58}{0.88} = -2.944\\]\nQue na distribuição de t fica:\n\n\n\n\n\n\n\n\n\nSe nosso nível de significancia \\(\\alpha = 0.05\\), então a probabilidade \\(p = 0.011 + 0.011 = 0.022\\) indica que devemos rejeitar \\(H_0\\) e aceitar que existe uma relação entre Riqueza de espécies e NAP.\n\n\n2.3.2 Análise de variância da regressão\nComo já dizemos, a estrutura de um modelo de regressão é dada por um componente sistemático expresso como função de \\(X\\) (\\(\\beta_0 + \\beta_1X_i\\)) e um componente aleatório expresso pelos resíduos do modelo (\\(\\epsilon_i\\)). A variação total em \\(Y\\) no modelo de regressão portanto, pode ser atribuída a ambos os efeitos de \\(X\\) e do resíduo. Estas quantias de variação podem mensuradas pelos somatório dos quadrados abaixo.\nSoma dos quadrados totais:\n\\(SQ_Y = \\sum{(Y_i - \\overline{Y})^2}\\)\nSoma dos quadrados da regressão:\n\\(SQ_{Regressão}= \\sum{(\\hat{Y_i} - \\overline{Y})^2}\\)\nE soma dos quadrados do resíduo:\n\\(SQ_{Resíduo}= \\sum{(Y_i - \\hat{Y_i})^2}\\)\nPode-se mostrar ainda que vale a expressão:\n\\[SQ_Y = SQ_{Regressão} + SQ_{Resíduo}\\] A decomposição destas quantias é conhecida partição das somas dos quadrados e nos permitem comparar a influência de \\(X\\) com a influência do puro acaso sobre a variabilidade em \\(Y\\). Se todos os pontos estiverem perfeitamente sobre a reta, então toda a variação em \\(Y\\) seria atribuída à influência de \\(X\\). Por outro lado, à medida que aumenta a distância média dos pontos acima e abaixo da curva, aumenta a parcela atribuída ao acaso.\n\n\n\n\n\n\n\n\n\nEstes componentes de variação podem ser organizados em uma Tabela de Análise de Variância (ANOVA). \\(n\\) se refere ao número de amostras.\n\n\n\n\n\n\n\n\n\n\n\nFonte de variação\nSQ\ngl\nQM\nF\np\n\n\n\n\nRegressão\n\\(SQ_{Regressão}\\)\n\\(gl_{Regressão}\\)\n\\(QM_{Regressão} = \\frac{SQ_{Regressão}}{gl_{Regressão}}\\)\n\\(\\frac{QM_{Regressão}}{QM_{Resíduo}}\\)\nProbabilidade associada à cauda da distribuição F\n\n\nResíduo\n\\(SQ_{Resíduo}\\)\n\\(gl_{Resíduo}\\)\n\\(QM_{Resíduo} = \\frac{SQ_{Resíduo}}{gl_{Resíduo}}\\)\n\n\n\n\nTotal\n\\(SQ_{Y}\\)\n\\(gl_{Y}\\)\n\\(QM_{Y} = \\frac{SQ_{Y}}{gl_{Y}}\\)\n\n\n\n\n\nAs coluna \\(gl\\) se refer aos graus de liberdade nos modelo de regressão, a semelhança do que discutimos para o teste t de Student. A coluna QM (Quadrado médio) apresenta os estimadores de variância da regressão (\\(QM_{Regressão}\\)), do resíduo (\\(QM_{Resíduo}\\)) e total (\\(QM_{Y}\\)).\n\n2.3.2.1 A distribuição F\nO valor de \\(F\\) na tabela se refere a distribuição de probabilidade F. Esta distribuição de probabilidades é esperada para a razão entre duas variâncias amostrais. No caso da regressão linear, estas são a variância da regressão (\\(QM_{Regressão}\\) no numerador) e a variância residual (\\(QM_{Resíduo}\\) no denominador). Diferente da distribuiçao t, a distribuição F tem um formato assimétrico, sendo que o grau de assimetria depende dos graus de liberdade do numerador e do denominador. O valor de \\(p\\) na tabela se refere à área sob a distribuição F, acima do valor de \\(F\\) calculado. Na ANOVA da regressão, um valor de \\(p < \\alpha\\) nos leva a rejeitar a hipótese nula e assumir que a variável \\(X\\) exerce algum efeito sobre \\(Y\\).\n\nO símbolo \\(F\\) foi dado em homenagem a Ronald Aylmer Fisher o estatístico e geneticista Britânico do início do séc. XX, que entre inúmeras outras contribuições, desenvolveu a Análise de Variância. Fisher é descrito como “a genius who almost single-handedly created the foundations for modern statistical science” (Halt 1998) e como “the single most important figure in 20th century statistics” (Efron 1998). Ver Ronald Aylmer Fisher.\n\n\n\n\n\n\n\n\n\n\nOs resultados da ANOVA para os dados da fauna praial nos dá os seguintes valores. Confira os cálculos.\n\n\n\n\n\nFonte de variação\nSQ\ngl\nQM\nF\np\n\n\n\n\nRegressão\n65.68\n1\n65.68\n8.64\n0.022\n\n\nResíduo\n53.21\n7\n7.60\nNA\nNA\n\n\nTotal\n118.89\n8\n14.86\nNA\nNA\n\n\n\n\n\nO valor de \\(p = 0.022\\) abaixo do nível de significância \\(\\alpha = 0.05\\), nos leva a rejeitar a hipótese nula em favor da alternativa, concluindo que o índice de exposição às ondas interfere sobre a riqueza da macro-fauna. O valor de \\(p\\) foi identico ao obtido no teste de hipóteses de \\(\\beta_1\\). No modelo de regressão linear simples isto é necessariamente verdadeiro, pois toda a variação associada à regressão é devida ao efeito do coeficiente \\(\\beta_1\\). Por outro lado, nos modelos de regressão múltipla, em que temos:\n\\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_1X_{i2} + \\cdots + \\beta_mX_{im} + \\epsilon_i\\]\nesta relação não é mais observada, pois existem múltiplos coeficientes agindo sobre a variação em \\(Y\\)."
  },
  {
    "objectID": "qmd/02-regressao/01-regressao_simples.html#coeficiente-de-determinação-r2",
    "href": "qmd/02-regressao/01-regressao_simples.html#coeficiente-de-determinação-r2",
    "title": "2  Regressão Linear Simples",
    "section": "2.4 Coeficiente de determinação \\(R^2\\)",
    "text": "2.4 Coeficiente de determinação \\(R^2\\)\nUma vez que toda a variação observada em Y pode ser alocada aos efeitos da reta de regressão e e do resíduo podemos fazer a seguinte questão:\n\nQual parcela da variação na Riqueza é explicada exclusivamente pelo modelo de regressão?\n\nEsta pergunta pode ser respondida calculando o que denominamos de coeficiente de determinção ou simplesmente \\(R^2\\):\n\\[R^2 = \\frac{SQ_{Regressão}}{SQ_Y} = 1 - \\frac{SQ_{Resíduo}}{SQ_Y}\\]\nO valor de \\(SQ_{Regressão}\\) mede a variação explicada exclusivamente pela regressão, \\(SQ_{Resíduo}\\) a variação residual e \\(SQ_Y\\) mede a variação total em \\(Y\\). Ao dividir \\(SQ_{Resíduo}\\) por \\(SQ_Y\\), o \\(r^2\\) nos informa sobre qual a fração da variação total é explicada somente pela reta de regressão.\nNpo exemplo da fauna praial:\n\\[R^2 = 1 - \\frac{53.11}{118.89} = 0.5533\\]\nO que significa que aproximadamente 55.33% da variação na riqueza é explicada pela variação no grau de exposição às ondas (NAP). Não sabemos a que se deve o restante da variação e, no contexto do modelo de regressão, assumimos ser uma variação aleatória inerente a cada observação (\\(\\epsilon_i\\)). Esta variação aleatória, como dito, segue uma distribuição normal com ponto central sobre a reta e variância data por \\(\\sigma^2\\). Este pressuposto é fundamental para a discussão do próximo ponto a respeito do intervalo de confiança de \\(Y\\)"
  },
  {
    "objectID": "qmd/02-regressao/01-regressao_simples.html#intervalo-de-confiança-de-y",
    "href": "qmd/02-regressao/01-regressao_simples.html#intervalo-de-confiança-de-y",
    "title": "2  Regressão Linear Simples",
    "section": "2.5 Intervalo de confiança de \\(Y\\)",
    "text": "2.5 Intervalo de confiança de \\(Y\\)\nComo nem todos os pontos caem perfeitamente sobre a reta, seria interessante que pudéssemos obter um intervalo de confiança de \\(Y\\) para um dado valor de \\(X\\). A amplitude deste intervalo irá depender da variância dos valores ajustados (\\(s^2_{Y|X}\\)) de \\(Y\\), calculada por:\n\\[s^2_{Y|X} = s^2(\\frac{1}{n} + \\frac{(X_i-\\overline{X})^2}{SQ_X})\\]\ndo modo que:\n\\[s_{Y|X} = \\sqrt{s^2(\\frac{1}{n} + \\frac{(X_i-\\overline{X})^2}{SQ_X})}\\]\nNote pela expressão acima que o \\(s_{Y|X}\\) diminui quanto:\n\na variância residual \\(s^2\\) diminui;\no tamanho amostral \\(n\\) aumenta.\no dado valor de \\(X_i\\) está próximo à média, pois neste caso \\((X_i-\\overline{X})\\) diminui.\n\nEncontrado \\(s_{Y|X}\\), o intervalo de confiança de \\(Y\\) é dado por:\n\\[IC_{Y} = \\hat{Y}\\pm t_{(\\alpha, n-2)} \\times s_{Y|X}\\]\n\n\n\nPara os dados da macrofauna, vamos exemplificar o cálculo de \\(IC_{95\\%}\\) para a \\(4^a\\) observação da tabela, em que Richness = 3 e NAP = 0.46.\nLembre-se que já estimamos anteriormente a variância residual destes dados (\\(s^2 = 7.59\\)). Como temos 9 observações, o valor de \\(t_{(\\alpha, n-2)} = 2.36\\), portanto:\n\\(s_{Y|X} = \\sqrt{s^2(\\frac{1}{n} + \\frac{(X_i-\\overline{X})^2}{SQ_X})} = 0.93\\)\nO valor estimado de riqueza neste ponto é 4.75, portanto:\n\\(IC_{Y} = \\hat{Y} \\pm t_{(\\alpha, n-2)} \\times s_{Y|X} = 4.75 \\pm 2.36 \\times 0.93\\)\n\\(IC_{Y} = 4.75 \\pm 2.19\\)\n\\(IC_{Y_{limite superior}} = 6.94\\)\n\\(IC_{Y_{limite inferior}} = 2.56\\)\nPodemos calcular intervalos destes para todos os pontos observados como expresso na tabela abaixo.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRichness\nNAP\n\\(\\hat{Y}\\)\n\\(s_{Y \\mid X}\\)\n\\(IC_{inferior}\\)\n\\(IC_{superior}\\)\n\n\n\n\n13\n-1.34\n9.40\n1.72\n5.34\n13.46\n\n\n8\n0.64\n4.29\n0.96\n2.02\n6.56\n\n\n4\n-0.20\n6.46\n1.03\n4.04\n8.88\n\n\n3\n0.46\n4.75\n0.93\n2.56\n6.94\n\n\n6\n0.73\n4.06\n0.99\n1.73\n6.39\n\n\n1\n2.22\n0.21\n1.90\n-4.29\n4.71\n\n\n1\n1.38\n2.38\n1.30\n-0.70\n5.46\n\n\n7\n-1.00\n8.52\n1.48\n5.02\n12.02\n\n\n3\n0.00\n5.94\n0.96\n3.67\n8.21\n\n\n\n\n\nE representá-los graficamente, juntamente com os valores ajustados de Y.\n\n\n\n\n\n\n\n\n\nNote que na figura acima, estão representados os valores observados de riqueza de espécies (em preto), os valores ajustados (azul) e os intervalos a 95% (vermelho). Os valores ajustados são aqueles utilizados para construir a reta de regressão. O intervalo não costuma ser representados por pontos individuais, mas por uma banda que delimita a área que restringe o intervalo de confiança ao nível \\(1 - \\alpha\\) como na figura abaixo.\n\n\n\n\n\n\n\n\n\nA banda mais estreita próxima ao ponto médio de \\(X\\), reflete o ponto comentado anteriormente, de que quanto mais próximo ao centro da distibuição de pontos, mais confiança temos sobre os limites máximos e mínimos que um valor de \\(Y\\) pode assumir. Do mesmo modo, esta confiança diminui à medida que nos aproximamos dos extremos dos valores observados em \\(X\\)."
  },
  {
    "objectID": "qmd/02-regressao/01-regressao_simples.html#pressupostos-da-regressão-linear-simples",
    "href": "qmd/02-regressao/01-regressao_simples.html#pressupostos-da-regressão-linear-simples",
    "title": "2  Regressão Linear Simples",
    "section": "2.6 Pressupostos da regressão linear simples",
    "text": "2.6 Pressupostos da regressão linear simples\nAo realizar uma regressão linear simples, devemos assumir como verdadeiros alguns pressupostos.\n\nO modelo linear descreve adequadamente a relação funcional entre \\(X\\) e \\(Y\\);\nCada par de observação \\((X,Y)\\) é independente dos demais;\nA variável \\(X\\) é medida sem erros;\nOs resíduos têm distribuição normal, e;\nA variância residual \\(\\sigma^2\\) é constante ao longo dos valores de \\(X\\).\n\n\n2.6.1 Relação funcional linear\nCaso a relação funcional entre \\(X\\) e \\(Y\\) assuma uma forma diferente de \\(Y_i = \\beta_0 + \\beta_1X_i\\), o modelo de regressão não é mais válido, pois a estimativa de erro irá conter, além do componente aleatório residual, um componente sistemático. Este componente terá efeito sobre influência sobre a predição do modelo, sobretudo nos extremos das observações. Por modelo linear, entendemos aqueles em que os coeficientes \\(\\beta\\) aparecem de forma aditiva. Modelos em que os componentes aparecem de outro modo na equação como potência ou no denominador de uma equação são exemplos de modelos não-lineares. Abaixo estão dois exemplos de relações não-lineares comumente observadas em fenômenos ambientais:\nEquação potência: \\(Y_i = \\beta_0 X_i^{\\beta_1}\\)\nModelo de Michaelis-Menten: \\(Y_i = \\frac{\\beta_0 X_i}{\\beta_1 + X_i}\\)\n\n\n2.6.2 Independência\nA falta de independência pode ocorrer como resultado do delineamento amostral inapropriado para a questão em teste. A falta de independência torna crítico o uso de uma distribuição de probabilidade para o cálculo do intervalo de confiança (distribuição \\(t\\)) e para o teste de hipóteses (distribuições \\(t\\) e \\(F\\) ). Casos clássicos de falta de independência são aqueles em que as observações são denominadas como pseudoréplicas (Hurlbert 1984). Após a publicação clássica de Hurlbert, muito tem sido dito sobre pseudoreplicação. Em experimentos de campo, a falta de independência ocorre geralmente como resultados da proximidade espacial entre as réplicas ou sobre séries temporais.\n\n\n2.6.3 Variável \\(X\\) é medida sem erros\nVeja que a parcela residual do modelo de regressão se refere à distância vertical de \\(Y_i\\), para um dados valor de \\(X\\). Isto implica que os níveis de \\(X\\) são previamente definidos. Quando existe variabilidade aleatória tanto em \\(Y\\) quanto em \\(X\\), o modelo correto para a estimativa dos parâmetros da regressão é conhecido como Modelo II de regressão. Este pressuposto é frequêntemente ignorado em delineamentos de regressão, sobretudo em estudos observacionais, o que não parece ser particularmente problemático.\n\n\n2.6.4 Distribuição normal dos resíduos\nAssim como no pressuposto de independência, assumir que os resíduos têm uma distribuição normal permite o uso da distribuição \\(F\\) pra o teste de hipótese e da distribuiçãio \\(t\\) para o cálculo do intervalo de confiança. Uma distribuição de erros diferente da distribuição normal terá influência sobre o cálculo da amplitude do intervalo de confiança.\n\n\n2.6.5 Variância residual constante\nCaso, a variância \\(\\sigma\\) não seja constante ao longo da reta de regressão, o cálculo do intervalo de confiança e o resultado do teste de hipóteses são afetados. Uma vez diagnosticada uma variância não-constante existem modelos de regressão que podem ser apicados para incorporar este efeito em suas estimativas (Zuur et al. 2009)."
  },
  {
    "objectID": "qmd/02-regressao/01-regressao_simples.html#diagnósticos-da-regressão",
    "href": "qmd/02-regressao/01-regressao_simples.html#diagnósticos-da-regressão",
    "title": "2  Regressão Linear Simples",
    "section": "2.7 Diagnósticos da regressão",
    "text": "2.7 Diagnósticos da regressão\nO diagnóstivo da regressão é composto por observações e testes que ajudam a decidirmos se a regressão linear foi um bom modelo para ajustar a um conjunto de dados particular. Um bom modelo neste contexto significa um modelo que atendeu aos pressuostos descritos acima. Esta verificação passa pela observação de padrões nos resíduos da regressão, ou seja, pela observação da parcela estocástica do modelo.\n\n2.7.1 Gráfico de resíduos\nO primeiro diagnóstico da regressão é conhecido como gráfico de resíduos, que consiste em um gráfico de dispersão entre os resíduos e o valor ajustado \\(\\hat{Y}\\). Abaixo estão os gráficos de resíduos que surge quando ajustamos uma reta a dados que apresentam uma relação linear, uma função potência, uma função assintótica e uma relação linear porém comm variância heterogênea.\n\n\n\n\n\nNas primeiras duas figuras, em que a relação é linear, vemos um padrão crescente de \\(Y\\) como função de \\(X\\) (figura da esquerda), em que os pontos estão aleatóriamente acima e abaixo da reta de regressão. Este padrão se reflete em um gráfico de resíduos (figura da direita) em que os pontos ficam aleatóriamente acima e abaixo de zero expressando resíduos positivos e negativos respectivamente. Em uma situação em que os pontos estivessem perfeitamente sobre a reta, os resíduos seriam todos iguais a zero e o gráfico de resíduos mostraria todos os pontos alinhados horizontalmente em zero.\nQuando a relação é potência e tentamos ajustar uma reta sobre, vemos que inicialmente os resíduos sao positivos, o seja, estão acima da reta. Os resíduos se tornam negativos no centro da nuvem de pontos e novamente positivos ao final do gráfico. Este padrão é mais evidente no gráfico de resíduos, que mostra um componente sistemático dos resíduos como fução do valor ajustado. Ao usar uma regressão linear neste caso, iríamos subestimar consistentemente os valores de Y nos extremos da figura e superestimlá-los no trecho central. Portanto, uma reta de regressão, quando ajustada a um conjunto de dados que expressa um padrão não-linear, não é capaz de isolar adequadamente as parcelas aleatórias e sistemáticas da relação entre \\(Y\\) e \\(X\\). Isto pode ser corrigido aplicando-se uma regressão não-linear aos dados.\nQuando a relação é assintótica, o resultado do ajuste foi inverso ao anterior. De fato, resultados análogos serão observados senpre que tentarmos ajustra uma regressão linear a dados que expressam padrões não-lineares.\nNo último exemplo (variância heterogênea) os pontos tendem a se afastar consistentemente da reta de regressão conforme aumentam os valores de \\(X\\). Isto denota que o pressuposto de variância \\(\\sigma^2\\) constante não é válido nesta relação. Isto pode ser corrigido aplicando-se um modelo de regressão linear com variância heterogênea.\n\n\n2.7.2 Histograma dos resíduos\nOutro diagnóstico da regressão consiste em fazer um histograma dos gráficos de resíduos. Um histograma, aproximadamente simétrico ao redor de zero o que sugere que o pressuposto de normalidade dos resíduos é válido neste caso. Existem testes formais de normalidade cmo o teste de Kolmogorov Smirnov ou o teste de Shapiro-Wilk.\n\n\n\n\n\n\n\n\n\nEfron, B. 1998. “R. A. Fisher in the 21st Century. Invited Paper Presented at the 1996 Ra Fisher Lecture.” Statistical Science 13 (2): 95–122.\n\n\nHalt, A. 1998. “A History of Mathematical Statistics.” Wiley, New York.\n\n\nHurlbert, Stuart H. 1984. “Pseudoreplication and the Design of Ecological Field Experiments.” Ecological Monographs 54 (2): 187–211.\n\n\nZuur, Alain, Elena N Ieno, Neil Walker, Anatoly A Saveliev, and Graham M Smith. 2009. Mixed Effects Models and Extensions in Ecology with r. Springer Science & Business Media."
  }
]